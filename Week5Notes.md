## Chapter 13: Nonlinear Classification Models

Kuhn begins by generally suggesting that these nonlinear models are particularly prone to be affected by non-informative or collinear predictors being used as inputs; thus, he suggests using these methods in conjunction with feature selection tools to improve the overall performance.

### Nonlinear discriminant analysis

- (***LDA**: Linear discriminant analysis*)
- **QDA**: Quadratic discriminant analysis
- **RDA**: Regularized discriminant analysis
- **MDA**: Mixture discriminant analysis

* The math: these methods all *minimize* the *total probability of misclassification* assuming that the data can truly be separated by hyperplanes (in LDA case), quadratic class boundaries (in QDA case), or some other type of higher order model boundary (RDA, MDA)

#### QDA & RDA

- recall that in linear discriminant analysis (LDA), assumption is that predictors have *homogenous variance*, and thus class boundaries were linear functions of the predictors (:confounded: a little confused on the consequence here)
- nonlinear DA allows for *class-specific*, rather than overall covariance. Consequence?
     - QDA: **class boundaries are quadratic curvilinear** rather than linear
          - same general data requirements, but more stringent: predictors can't have too much collinearity, predictors must be fewer than observations (within a class)
     - RDA: **class boundaries are somewhere between linear and quadratic**
          - bridge between LDA and QDA
          
Are the class boundaries *linearly* separable? --> LDA

Are the class boundaries *quadratically* separable? --> QDA

Are the class boundaries *somewhere in between linear and quadratic?* --> RDA

- Friedman proposed the following equation:
<p align="center"><img src="http://latex.codecogs.com/gif.latex?\tilde{\Sigma}_{\ell}(\lambda)&space;=&space;\lambda\Sigma_{\ell}&plus;(1-\lambda)\Sigma"></a>

- where <img src="http://latex.codecogs.com/gif.latex?\Sigma_{\ell}"></a> is the covariance matrix of the <img src="http://latex.codecogs.com/gif.latex?\ell"></a>th class and <img src="http://latex.codecogs.com/gif.latex?\Sigma"></a> is the pooled covariance matrix across all classes

### MDA
- allows each class to be represented by multiple multivariate normal distributions :confounded:
- tuning parameter: **number of distributions per class** (doesn't have to be equal across classes); these are known as **subclasses**
     - optimal number of subclasses = that which maximizes area under the ROC curve
     
#### 13.2 Neural networks
- can handle multiple output classes, not just binary
- nonlinear transform is carried out within the hidden units of the network (usually using sigmoid function) in order to place the predicitions on a scale from 0 to 1
- however, even though predictions are between 0 and 1, they are not probability-like since they do not add up to one!
<p align="center">
   <img width="712" height="584"src="https://github.com/jclauneuro/Kuhn_AppliedPredictiveModeling/blob/master/figures/fig13.3.png">
</p>

- __*Softmax*__ transform is used to ensure the outputs of network comply with this constarint

<p align="center"><img src="http://latex.codecogs.com/gif.latex?\large&space;f_{i\ell}^{\ast}(x)=\frac{e^{f_{i\ell}(x)}}{\sum_{\l}e^{f_{i\ell}(x)}}"></a>

- where <img src="http://latex.codecogs.com/gif.latex?\large&space;f_{i\ell}(x)"></a> is the model prediction of the <img src="http://latex.codecogs.com/gif.latex?\ell"></a>th class and the *i*th sample.
- neural network will optimize the sum of squared errors across samples __AND__ classes:
<p align="center"><img src="http://latex.codecogs.com/gif.latex?\large&space;\sum_{\ell=1}^{C}\sum_{i=1}^{n}(y_{i\ell}-f_{i\ell}^{\ast}(x))^{2}"></a>

- where <img src="http://latex.codecogs.com/gif.latex?\large&space;y_{i\ell}"></a> is the 0/1 indicator for the class <img src="http://latex.codecogs.com/gif.latex?\ell"></a>.
- thus, the class with the largest predicted value would be used to classify the sample
- alternatively, parameter estimates can be found that maximize the likelihood of the __Bernoulli__ distribution:
<p align="center"><img src="http://latex.codecogs.com/gif.latex?\large&space;\sum_{\ell=1}^{C}\sum_{i=1}^{n}y_{i\ell}\:\textup{ln}\:f_{i\ell}^{\ast}(x)"></a>

- this is called __*cross-entropy*__ and has been suggested to be able to more accurately estimate small probabilites than those generated by the squared-error function
- neural networks for classification have a significant potential for over-fitting
   * when optimizing the sums of squares or entropy, weight decay attentuates the size of the parameter estimates (much smoother classification boundaries)
- to counteract this, the class probabilites estimates are averaged across networks and these average values would be used to classify

#### 13.3 Flexible discriminant analysis
- a way of *extending* linear discriminant analysis by using a mathematical formulation of the problem that is different from the classic LDA [Hastie1994]
* this then allows some of the tools used for regression we had previously covered (e.g. lasso, ridge regression, MARS) to create discriminant variables
* this is a *conceptual framework*
- Can combo with MARS, e.g.
- INTERPRETABLE :confounded: The exact effect of predictors on the model can be spelled out
     - See fig 13.8 on pg 342
     - BUT, since predictors are all on dif scales, difficult to determine which ones have MOST impact on outcome
     
<p align="center">
     <img src="https://github.com/jclauneuro/Kuhn_AppliedPredictiveModeling/blob/master/figures/fig13.8.png">
</p>

#### 13.4 Support Vector Machines
* alternative class of statistical models identifying model performance (for regression or classification) based on the concept of the *margin*
- this is a lot more intuitive for classification than regression
- **Reminder**: *basically you find points that are most difficult to classify, and draw a line between them... do this over and over until you've classified your data into parts*
- "one of the most flexible and effective machine learning tools available"

**Margin** 

*When classes are __clearly separable__...*

- accuracy not always the best metric for model effectiveness, especially for clearly separable classes. 
     - alternative: **margin**
- :book: **margin**: distance between the classification boundary and the nearest training set point
- :book: **support vectors** are points that define/fall along the margin (these are a subset of the training set)
- :book: **maximum margin classifier**: slope and intercept of boundary that maximize the buffer between the boundary and the data
- new observations are classified according to *which support vectors* they are closest to; and are classified on the same side of the support vector machine (the prediction equation) as these points
     - therefore, new observations are classified only based on their relative distance to a subset of the training points (the support vectors). See fig 13.10
     
*When classes are __not__ clearly separable*...

- tuning parameters: 
     - **cost value** that penalizes the margin for points that fall on it or on the wrong side of it
     - **kernel function** (modulate the support vectors in some nonlinear way -- e.g. polynomial; think of analogy to image kernels for blurring themselves)
          - Kinds of kernels: polynomial, radial basis function, hyperbolic tangent
          - the kernel "trick" enables SVMs to produce flexible decision boundaries
- higher cost, higher kernel --> better fit (but also possible overfit)

#### 13.5 K-Nearest neightbours
- recall: for regression, KNN predicts new sample using k-closest samples from training set
- Same idea for classification: class probability estimates for new sample are calculated as the proportion of training set neighbours in each class
     - the new samples predicted class is the class with the highest probability estimate (ties determined at random or by looking at K+1 nearest neighbour)
     
- overfitting can arise if *too few neighbours*
     - use cross-validation or resampling approach to determine optimal value of k :confounded:
     
#### 13.6 Naive Bayes
- "Bayes' Rule answers the question: *based on the predictors that we have observed, what is the probability that the outcome is class Cl?*"

*Key components*:

- :confounded: **prior probability of the outcome**: based on what we've seen, what would we expect the probabiliity of the class to be? (e.g., disease prevalence)
- :confounded: **probability of the predictor values**: how well does the new predicted sample pattern with the observed sample?
- :confounded: **conditional probability**: what's the probability of observing the predictor values?

Naive bayes: **simplifies** probabilities of predictor values by **assuming independence of predictors** (which is usually not true)

<p align="center">
     <img src="https://github.com/jclauneuro/Kuhn_AppliedPredictiveModeling/blob/master/figures/Fig13.15_naive_bayes_classification.png">
</p>

## Chapter 14: Classification Trees and Rule-Based Models
- fall within the family of tree-based models and consist of nested __if-then__ statements:
```
     if Predictor B >= 0.197:
     |    if Predictor A>= 0.13:
     |         Class = 1
     |    else:
     |         Class = 2
     else:
          Class = 2
``` 
- they can be highly interpretable, handle many types of predictors as well as missing data
- suffer from model instability and may not produce optimal predictive performance

### 14.1 Basic Classification Trees
- goal is paritition the data into smaller, more homogeneous groups
     * __Homogeneity__ means that the nodes of the split are more pure (contain larger proportion of one class in each node)
- __purity__, in classification, is defined as maximizing accuracy or miimizing misclassification error
- __Gini__ index can be used to measure purity:

<p align="center">
<i>p</i><sub>1</sub>(1- <i>p</i><sub>1</sub>) + <i>p</i><sub>2</sub>(1 - <i>p</i><sub>2</sub>)
</p>

- where *p* <sub>1</sub> and *p* <sub>2</sub> are the Class 1 and Class 2 probabilities
- Gini index minimized when either class probabilities is driven to zero, meaning the node is pure with respect to one of the classes
- Gini index is maximized when *p*<sub>1</sub> = *p*<sub>2</sub>, this is a case when the node is least pure
- first samples sorted based on predictor values, split points are then the midpoints between each unique predictor value
- if the response is binary, rpoduces a 2x2 contingency table at each split point

|                 | Class 1    | Class 2   |            |
| :-------------- |:----------:| :--------:| :---------:|
| B > 0.197       | n11 = 91   | n12 = 30  | n1 = 121   |
| B < 0.197       | n21 = 20   | n22 = 67  | n2 = 87    |
|                 | 111        | 104       | n = 208    |
     
```
B > 0.197: 2*(91/121)(30/121) = .373
B < 0.197: 2*(20/87)(67/87) = .354

Combined = 2*[(n11/n)(n12/n1) + (n21/n)(n22/n2)]
         = 2*[(91/208)(30/121) + (20/208)(67/87)] 
         = .365
```

### 14.2 Rule-Based Models
- unlike trees a sample may be predicted from a set of rules

#### C4.5 Rules
- an unpruned tree is created, then each path through the tree is collapsed into an individual rule
- each rule is evaluated individually to assess wether it can be genereralized by eliminating terms in the conditional statement
- For each rule:
     1. the model first calculates a baseline pessimistic error rate.
     2. the model removes each condition in the rule in isolation, the pessimistic error rate is re-calculated each time.
     3. if any error rate is smaller than the baseline, the condition associated with the smallest error rate is removed.
     4. The process repeats until all conditions are above the baseline rate or all conditions are removed (rule pruned from model).
     
#### PART
- C4.5Rules assumes initial rules are developed simultaneously then post-processing occurs
- however, rules can be created incrementally
- a new rule can can adapt to the previous set of rules and may more effectively capture important trends
- Steps in PART:
     1. Pruned C4.5 tree is created
     2. the path that covers the most samples is retained as a rule
     3. samples covered by the rule are disgarded.
     4. repeat until all samples are covered by at least one rule.
     
### 14.3 Bagged Trees
- each model in the ensemble is used to predict the class of the new sample
- the total number of votes for each class (cast by each model) is divided by the total number of models in ensemble to produce predicted probability
- the new sample is then classified into thegroup that has the most votes

### 14.4 Random Forests
- each tree in the forest casts a vote for the classification of a new sample, and the proportion of votes in each class across the ensemble is the predicted probability vector
- during training, predictors are randomly sampled at each split to de-correlate the trees in the forest
- recommendations:
     * tuning amount of randomly sampled predictors: start with 5 values somewhat evenly spaced across the range 2 to # of predictors
     * start with an ensemble of 1000 trees and increase number if performance is not yet close to a plateau
     
### 14.5 Boosting
- many weak classifiers are combined into a strong classifier

#### AdaBoost
- generates a sequence of weak classifiers, where at each iteration the algorithm finds the best classifier based on the current sample weights
- samples that are incorrectly classified receive more weight in the next iteration, samples correctly classified receive less weight
- this means samples that are more difficult to classify receive increasingly larger weights until algorithm identifies model that correctly classifies these samples
- would this not mean over-fitting?? Just depends on how close your dataset is to representing the population

#### Stochastic Gradient Boosting
- []
### 14.6 C5.0 Rules
- suped-up C4.5 that has boosting and unequal costs for different types of errors
#### C5.0 Classification Trees
- improvements that are likely to generate smaller (simpler!) trees
- algorithm will combine non-occurring conditions for splits with several categories
- conducts a final global pruning procedure that attempts to remove the sub-trees with a cost-complexity approach

#### C5.0 Classification Rules
- initial tree is grown, collpased into rules, then individual rules are simplified via pruning and a global procedure is used on the entire set to potentially reduce the number of contituent rules
- C5.0 does not order the rules like in C4.5, instead it uses all active rules
- the active rules all vote for the likley class, votes for each class are weighted by the confidence values and the class associated with the highest vote is used
- the predicted confidence value is the one associated with the most specific active rule.
- __*utility*__ bands can be created (measured as the increase in error that occurs when the rule is removed) and the model removes the rule with the smallest utility and recomputes the utilities for the other rules

#### C5.0 Boosting
- models are fit sequentially and each iteration adjusts the case weights based on the accuracy of a samples prediction
- however, C5.0 attempts to create trees the same size as the first tree by coercing the trees to have about the same number of terminal nodes
     * previous boosting techniques treated the tree complexity as a tuning parameter
- the model combines the predictions from the constituent trees differently than AdaBoost
     * each boosted model calculates confidence values for each class and a simple average of these values is calculated
     * the class with the largest confidence values is selected (stage weights are not calculated)
- the model will automatically stop boosting if:
     1. the model is very effective (sum of the weights for the missclassified samples is less than 0.10)
     2. the model is highly ineffective (the average weight of incorrect samples is greater than 50%)
- C5.0 uses different weighting scheme during model training

#### C5.0 Other Aspects of the Model
- measures predictor importance by determining the percentage of training set samples that fall into all the terminal nodes after the split
- has option to *winnow* or remove predictors:
     * an initial algorithm uncovers which predictors have a relationship with the outcome, and final model is created from only the important predictors
- to do this a __*Winnowing Tree*__ is created: training set is split in half and a tree is created for the purpose of evaluating the utility of the predictors.
- Two procedures that characterize the importance of each predictor to the model:
     1. predictors considered unimportant if they are not in any split in the winnowing tree
     2. half of samples not used to create winnowing tree are used to estimate error rate of tree. 
- once list of tentative non-informative predictors is established, C5.0 recreates the tree
     * if error rate has become worse, the winnowing process is disabled and no predictors are excluded
     * if it improves then C5.0 continues to train using all samples but with __ONLY__ the predictors that survived winnowing
     
### 14.7 Comparing Two Encodings of Categorical Predictors

## Chapter 16: Remedies for Severe Class Imbalance
- when modelling discrete classes, the relative frequency of classes can have significant impact on effectiveness of the model
- imbalance occurs when one or more classes have very low proportions in training data as compared to other classes

### 16.2 The Effect of Class Imbalance
- can result in good specificity but poor sensitivity
- can also effect predicted class probabilities

### 16.3 Model Tuning
- tune model to maximize the accuracy of the minority class(es)
- minor improvement in sensitivity comes at no cost to specificity

### 16.4 Alternative Cutoffs
- when there are two possible outcome categories, determine alternative cutoffs for the predicted probabilities which effectively changes the definition of predicted event
- use the ROC curve to determine appropriate balance between sensitivity and specificity

### 16.5 Adjusting Prior Probabilites
- models such as naive Bayes and discriminant analysis classifiers use prior probabilities
- models typically derive value of priors from training data so use more balanced priors or a balanced training set

### 16.6 Unequal Case Weights
- many predictive models for classification use case weights, where each individual data point can be given more emphasis in the model training phase
- thus, increase the weights for the samples in the minority classes

### 16.7 Sampling Methods
- simplest method would be to select a training set to have roughly equal event rates during the initital data collection
- however, if the training set is sampled to be balanced, the test set should be sampled to be more consistent with the state of nature and should reflect the imbalance so that honest estimates can be computed
- if a prior knowledge of class imbalances is not known then this method can not be used
- Post-hoc approaches that can be used: up-sampling and down-sampling 
- Up-sampling:
     * technique that simulates or *imputes* additional data points to improve balance across classes
     * approach would be to sample with replacement from minority class until each class has same number
- Down-sampling:
     * technique that reduces the number of samples to improve the balance
     * select data points from majority class so that majority class is same size of minority class
     * One approach is to randomly sample the majority class
     * another apporach is to bootstrap across all classes, which has the advantage of being able to run many times so that estimate of variation can be obtained about the down-sampling
- __Synthetic Minority Over-sampling Technique (SMOTE)__ uses both up-sampling and down-sampling, depending on the class, and has three operational parameters
     * amount of up-sampling, amount of down-sampling, and number of neighbors that are used to impute new cases
- SMOTE up-sampling:
     * data point is randomly selected from minority class and it's *K*NNs are determined
     * new synthetic data point is random combination of predictors of the randomly selected datapoint and its neighbors
     
### 16.8 Cost-Sensitive Training
- instead of optimizing accuracy or impurity, can optimize cost or loss function that differentially weights specific types of errors
- for instance, misclassifying true events (false negatives) is *X* times as costly as incorrectly predicting nonevents (false positives)



